{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERTv2: Indexing & Search Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the relevant classes. As we'll see below, `Indexer` and `Searcher` are the key actors here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow here assumes an IR dataset: a set of queries and a corresponding collection of passages.\n",
    "\n",
    "The classes `Queries` and `Collection` provide a convenient interface for working with such datasets.\n",
    "\n",
    "We will use the *dev set* of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely `lifestyle:dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-30 09:57:15--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 405924985 (387M) [application/octet-stream]\n",
      "Saving to: ‘downloads/colbertv2.0.tar.gz’\n",
      "\n",
      "colbertv2.0.tar.gz  100%[===================>] 387.12M  5.01MB/s    in 73s     \n",
      "\n",
      "2022-09-30 09:58:28 (5.32 MB/s) - ‘downloads/colbertv2.0.tar.gz’ saved [405924985/405924985]\n",
      "\n",
      "colbertv2.0/\n",
      "colbertv2.0/artifact.metadata\n",
      "colbertv2.0/vocab.txt\n",
      "colbertv2.0/tokenizer.json\n",
      "colbertv2.0/special_tokens_map.json\n",
      "colbertv2.0/tokenizer_config.json\n",
      "colbertv2.0/config.json\n",
      "colbertv2.0/pytorch_model.bin\n",
      "--2022-09-30 09:58:36--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3576167599 (3.3G) [application/octet-stream]\n",
      "Saving to: ‘downloads/lotte.tar.gz’\n",
      "\n",
      "lotte.tar.gz        100%[===================>]   3.33G  4.98MB/s    in 11m 17s \n",
      "\n",
      "2022-09-30 10:09:53 (5.04 MB/s) - ‘downloads/lotte.tar.gz’ saved [3576167599/3576167599]\n",
      "\n",
      "lotte/\n",
      "lotte/science/\n",
      "lotte/science/test/\n",
      "lotte/science/test/questions.search.tsv\n",
      "lotte/science/test/questions.forum.tsv\n",
      "lotte/science/test/collection.tsv\n",
      "lotte/science/test/qas.forum.jsonl\n",
      "lotte/science/test/metadata.jsonl\n",
      "lotte/science/test/qas.search.jsonl\n",
      "lotte/science/dev/\n",
      "lotte/science/dev/questions.search.tsv\n",
      "lotte/science/dev/questions.forum.tsv\n",
      "lotte/science/dev/collection.tsv\n",
      "lotte/science/dev/qas.forum.jsonl\n",
      "lotte/science/dev/metadata.jsonl\n",
      "lotte/science/dev/qas.search.jsonl\n",
      "lotte/writing/\n",
      "lotte/writing/test/\n",
      "lotte/writing/test/questions.search.tsv\n",
      "lotte/writing/test/questions.forum.tsv\n",
      "lotte/writing/test/collection.tsv\n",
      "lotte/writing/test/qas.forum.jsonl\n",
      "lotte/writing/test/metadata.jsonl\n",
      "lotte/writing/test/qas.search.jsonl\n",
      "lotte/writing/dev/\n",
      "lotte/writing/dev/questions.search.tsv\n",
      "lotte/writing/dev/questions.forum.tsv\n",
      "lotte/writing/dev/collection.tsv\n",
      "lotte/writing/dev/qas.forum.jsonl\n",
      "lotte/writing/dev/metadata.jsonl\n",
      "lotte/writing/dev/qas.search.jsonl\n",
      "lotte/recreation/\n",
      "lotte/recreation/test/\n",
      "lotte/recreation/test/questions.search.tsv\n",
      "lotte/recreation/test/questions.forum.tsv\n",
      "lotte/recreation/test/collection.tsv\n",
      "lotte/recreation/test/qas.forum.jsonl\n",
      "lotte/recreation/test/metadata.jsonl\n",
      "lotte/recreation/test/qas.search.jsonl\n",
      "lotte/recreation/dev/\n",
      "lotte/recreation/dev/questions.search.tsv\n",
      "lotte/recreation/dev/questions.forum.tsv\n",
      "lotte/recreation/dev/collection.tsv\n",
      "lotte/recreation/dev/qas.forum.jsonl\n",
      "lotte/recreation/dev/metadata.jsonl\n",
      "lotte/recreation/dev/qas.search.jsonl\n",
      "lotte/lifestyle/\n",
      "lotte/lifestyle/test/\n",
      "lotte/lifestyle/test/questions.search.tsv\n",
      "lotte/lifestyle/test/questions.forum.tsv\n",
      "lotte/lifestyle/test/collection.tsv\n",
      "lotte/lifestyle/test/qas.forum.jsonl\n",
      "lotte/lifestyle/test/metadata.jsonl\n",
      "lotte/lifestyle/test/qas.search.jsonl\n",
      "lotte/lifestyle/dev/\n",
      "lotte/lifestyle/dev/questions.search.tsv\n",
      "lotte/lifestyle/dev/questions.forum.tsv\n",
      "lotte/lifestyle/dev/collection.tsv\n",
      "lotte/lifestyle/dev/qas.forum.jsonl\n",
      "lotte/lifestyle/dev/metadata.jsonl\n",
      "lotte/lifestyle/dev/qas.search.jsonl\n",
      "lotte/evaluate_lotte_rankings.py\n",
      "lotte/technology/\n",
      "lotte/technology/test/\n",
      "lotte/technology/test/questions.search.tsv\n",
      "lotte/technology/test/questions.forum.tsv\n",
      "lotte/technology/test/collection.tsv\n",
      "lotte/technology/test/qas.forum.jsonl\n",
      "lotte/technology/test/metadata.jsonl\n",
      "lotte/technology/test/qas.search.jsonl\n",
      "lotte/technology/dev/\n",
      "lotte/technology/dev/questions.search.tsv\n",
      "lotte/technology/dev/questions.forum.tsv\n",
      "lotte/technology/dev/collection.tsv\n",
      "lotte/technology/dev/qas.forum.jsonl\n",
      "lotte/technology/dev/metadata.jsonl\n",
      "lotte/technology/dev/qas.search.jsonl\n",
      "lotte/pooled/\n",
      "lotte/pooled/test/\n",
      "lotte/pooled/test/questions.search.tsv\n",
      "lotte/pooled/test/questions.forum.tsv\n",
      "lotte/pooled/test/collection.tsv\n",
      "lotte/pooled/test/qas.forum.jsonl\n",
      "lotte/pooled/test/qas.search.jsonl\n",
      "lotte/pooled/dev/\n",
      "lotte/pooled/dev/questions.search.tsv\n",
      "lotte/pooled/dev/questions.forum.tsv\n",
      "lotte/pooled/dev/collection.tsv\n",
      "lotte/pooled/dev/qas.forum.jsonl\n",
      "lotte/pooled/dev/qas.search.jsonl\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p downloads/\n",
    "\n",
    "# ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/colbertv2.0.tar.gz -C downloads/\n",
    "\n",
    "# The LoTTE dev and test sets (3.4GB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/lotte.tar.gz -C downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 10:58:21] #> Loading the queries from downloads/lotte/lifestyle/dev/questions.search.tsv ...\n",
      "[Sep 30, 10:58:21] #> Got 417 queries. All QIDs are unique.\n",
      "\n",
      "[Sep 30, 10:58:21] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 417 queries and 268,893 passages'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = 'downloads/lotte'\n",
    "dataset = 'lifestyle'\n",
    "datasplit = 'dev'\n",
    "\n",
    "queries = os.path.join(dataroot, dataset, datasplit, 'questions.search.tsv')\n",
    "collection = os.path.join(dataroot, dataset, datasplit, 'collection.tsv')\n",
    "\n",
    "queries = Queries(path=queries)\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded 417 queries and 269k passages. Let's inspect one query and one passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what kind of coffee do you put in a coffee maker?\n",
      "\n",
      "Just call or e-mail them and ask for a better price. More often than not you'll get a discount. Works on appliances, materials, parts and everything else. Exception: big chain stores. My better half does this for most of the things we buy for the house. Maybe her voice plays a role here -- hard to say :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[72])\n",
    "print()\n",
    "print(collection[123456])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "For efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
    "\n",
    "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index.\n",
    "\n",
    "(With four Titan V GPUs, indexing should take about 13 minutes. The output is fairly long/ugly at the moment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'downloads/colbertv2.0'\n",
    "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Sep 30, 10:58:32] #> Note: Output directory /future/u/udingank/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits already exists\n",
      "\n",
      "\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "nranks = 4 \t num_gpus = 4 \t device=3\n",
      "[Sep 30, 10:58:53] [3] \t\t #> Encoding 16879 passages..\n",
      "nranks = 4 \t num_gpus = 4 \t device=0\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"downloads\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": {\n",
      "        \"provenance\": \"downloads\\/lotte\\/lifestyle\\/dev\\/collection.tsv\"\n",
      "    },\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"lifestyle.dev.2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/future\\/u\\/udingank\\/ColBERT\\/docs\\/experiments\",\n",
      "    \"experiment\": \"notebook\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-09\\/30\\/10.58.17\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 4,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 4\n",
      "}\n",
      "[Sep 30, 10:58:53] [0] \t\t # of sampled PIDs = 90887 \t sampled_pids[:3] = [218428, 5331, 156573]\n",
      "nranks = 4 \t num_gpus = 4 \t device=2\n",
      "[Sep 30, 10:58:54] [2] \t\t #> Encoding 23247 passages..\n",
      "[Sep 30, 10:58:54] [0] \t\t #> Encoding 25264 passages..\n",
      "nranks = 4 \t num_gpus = 4 \t device=1\n",
      "[Sep 30, 10:58:54] [1] \t\t #> Encoding 25497 passages..\n",
      "[Sep 30, 10:59:56] [1] \t\t avg_doclen_est = 151.47828674316406 \t len(local_sample) = 25,497\n",
      "[Sep 30, 10:59:56] [3] \t\t avg_doclen_est = 151.47828674316406 \t len(local_sample) = 16,879\n",
      "[Sep 30, 10:59:56] [0] \t\t avg_doclen_est = 151.47828674316406 \t len(local_sample) = 25,264\n",
      "[Sep 30, 10:59:56] [2] \t\t avg_doclen_est = 151.47828674316406 \t len(local_sample) = 23,247\n",
      "[Sep 30, 11:00:27] [0] \t\t Creaing 65,536 partitions.\n",
      "[Sep 30, 11:00:27] [0] \t\t *Estimated* 40,731,450 embeddings.\n",
      "[Sep 30, 11:00:27] [0] \t\t #> Saving the indexing plan to /future/u/udingank/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/plan.json ..\n",
      "Clustering 13702168 points in 128D to 65536 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.78 s\n",
      "  Iteration 19 (330.99 s, search 317.03 s): objective=3.40537e+06 imbalance=1.248 nsplit=0       \n",
      "[Sep 30, 11:06:16] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:18] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.036, 0.037, 0.035, 0.033, 0.034, 0.036, 0.035, 0.034, 0.034, 0.034, 0.034, 0.034, 0.037, 0.037, 0.035, 0.037, 0.031, 0.036, 0.035, 0.034, 0.034, 0.036, 0.034, 0.034, 0.034, 0.034, 0.035, 0.034, 0.038, 0.037, 0.036, 0.04, 0.037, 0.034, 0.034, 0.034, 0.036, 0.035, 0.035, 0.044, 0.036, 0.033, 0.035, 0.036, 0.036, 0.034, 0.032, 0.038, 0.037, 0.035, 0.034, 0.035, 0.039, 0.036, 0.034, 0.034, 0.038, 0.038, 0.044, 0.034, 0.035, 0.037, 0.035, 0.036, 0.038, 0.037, 0.039, 0.035, 0.034, 0.035, 0.037, 0.031, 0.033, 0.036, 0.035, 0.036, 0.037, 0.036, 0.036, 0.037, 0.039, 0.035, 0.036, 0.037, 0.033, 0.036, 0.036, 0.033, 0.032, 0.039, 0.035, 0.039, 0.035, 0.037, 0.036, 0.036, 0.039, 0.033, 0.036, 0.035, 0.033, 0.037, 0.035, 0.036, 0.038, 0.033, 0.036, 0.033, 0.035, 0.035, 0.036, 0.035, 0.037, 0.035, 0.036, 0.035, 0.038, 0.036, 0.034, 0.037, 0.034, 0.035, 0.037, 0.036, 0.033, 0.037, 0.036, 0.034]\n",
      "[Sep 30, 11:06:19] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Sep 30, 11:06:19] #> Got bucket_cutoffs = tensor([-0.0287,  0.0001,  0.0291], device='cuda:0') and bucket_weights = tensor([-0.0504, -0.0134,  0.0136,  0.0508], device='cuda:0')\n",
      "[Sep 30, 11:06:19] avg_residual = 0.03564453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:06:19] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:19] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:19] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:06:20] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:20] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:20] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:06:20] [3] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:06:21] [1] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:06:21] [2] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:07:19] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 3,779,083 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:02, 62.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:07:22] [0] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:07:25] [3] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:07:25] [2] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:07:25] [1] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:08:22] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 3,953,755 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [02:06, 63.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:08:26] [0] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:08:27] [2] \t\t #> Encoding 18893 passages..\n",
      "[Sep 30, 11:08:28] [1] \t\t #> Encoding 25000 passages..\n",
      "[Sep 30, 11:09:25] [0] \t\t #> Saving chunk 8: \t 25,000 passages and 3,698,831 embeddings. From #200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [03:08, 62.80s/it]\r",
      "3it [03:08, 62.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:09:34] [0] \t\t #> Checking all files were saved...\n",
      "[Sep 30, 11:09:34] [0] \t\t Found all files!\n",
      "[Sep 30, 11:09:34] [0] \t\t #> Building IVF...\n",
      "[Sep 30, 11:09:34] [0] \t\t #> Loading codes...\n",
      "[Sep 30, 11:09:34] [0] \t\t Sorting codes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/11 [00:00<?, ?it/s]\r",
      "100%|██████████| 11/11 [00:00<00:00, 65.66it/s]\r",
      "100%|██████████| 11/11 [00:00<00:00, 65.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:09:38] [0] \t\t Getting unique codes...\n",
      "[Sep 30, 11:09:38] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Sep 30, 11:09:38] #> Building the emb2pid mapping..\n",
      "[Sep 30, 11:09:39] len(emb2pid) = 40756586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 65536/65536 [00:03<00:00, 16630.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:09:44] #> Saved optimized IVF to /future/u/udingank/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/ivf.pid.pt\n",
      "[Sep 30, 11:09:44] [0] \t\t #> Saving the indexing metadata to /future/u/udingank/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "#> Joined...\n",
      "#> Joined...\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=4, experiment='notebook')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/future/u/udingank/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.get_index() # You can get the absolute path of the index, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
    "\n",
    "We can use the `queries` set we loaded earlier — or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:14:29] #> Loading collection...\n",
      "0M \n",
      "[Sep 30, 11:14:34] #> Loading codec...\n",
      "[Sep 30, 11:14:34] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:14:34] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Sep 30, 11:14:35] #> Loading IVF...\n",
      "[Sep 30, 11:14:35] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 612.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 30, 11:14:35] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00, 10.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# To create the searcher using its relative name (i.e., not a full path), set\n",
    "# experiment=value_used_for_indexing in the RunConfig.\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name)\n",
    "\n",
    "\n",
    "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
    "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
    "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
    "# but you can gain more extensive search by setting larger values of k or\n",
    "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> what are white spots on raspberries?\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . what are white spots on raspberries?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2024,  2317,  7516,  2006, 20710,  2361, 20968,\n",
      "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\t [1] \t\t 26.0 \t\t You've got a heat problem, this is UV damage or excessive heat during the ripening phase and referred to as White Drupelet syndrome (white spot). It's quite common on Raspberries during the final crops of the year as summer heat increases. Also occurs in blackberries. Last year, we had issues in the US Pacific Northwest with it, only a handful at end of season this year as we've had a cold, damp summer.\n",
      "\t [2] \t\t 25.8 \t\t White Drupelet syndrome (white spot) has the cell fully formed, no powdery residue, but the color is white instead of the normal color. It's caused by excessive sunlight (UV damage) or heat and is typically found on late season set raspberries. I view it as a normal end-of-crop phenomenon that indicates cane maintenance will be in order soon for next year's crop. Berries are entirely edible, you just won't be selling them. The first berry in the top row is a gold standard reference. White Drupelet Disorder If your berries got damp, powdery mildew can also be a late season condition. The powder will be gray and the berries mushy.\n",
      "\t [3] \t\t 19.9 \t\t The white spots are a mixture of pesticide residue and hard water stains from overhead watering in the grower's greenhouse. Wiping the leaves with a damp cloth is usually sufficient to remove the residue.\n"
     ]
    }
   ],
   "source": [
    "query = queries[37]   # or supply your own query\n",
    "\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Search\n",
    "\n",
    "In many applications, you have a large batch of queries and you need to maximize the overall throughput. For that, you can use the `searcher.search_all(queries, k)` method, which returns a `Ranking` object that organizes the results across all queries.\n",
    "\n",
    "(Batching provides many opportunities for higher-throughput search, though we have not implemented most of those optimizations for compressed indexes yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 417/417 [00:03<00:00, 136.40it/s]\n"
     ]
    }
   ],
   "source": [
    "rankings = searcher.search_all(queries, k=5).todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24367, 1, 16.1875),\n",
       " (25089, 2, 16.015625),\n",
       " (35359, 3, 16.015625),\n",
       " (131623, 4, 15.9765625),\n",
       " (3789, 5, 15.9375)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings[30]  # For query 30, a list of (passage_id, rank, score) for the top-k passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
